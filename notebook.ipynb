{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtyping import TensorType\n",
    "\n",
    "class SingleHeadAttention(nn.Module):\n",
    "        \n",
    "    def __init__(self, model_dim: int, head_size: int):\n",
    "        super().__init__()\n",
    "        self.key_layer = nn.Linear(model_dim, head_size, bias=False)\n",
    "        self.query_layer = nn.Linear(model_dim, head_size, bias=False)\n",
    "        self.value_layer = nn.Linear(model_dim, head_size, bias=False)\n",
    "    \n",
    "    def forward(self, embedded: TensorType[float]) -> TensorType[float]:\n",
    "        K = self.key_layer(embedded)\n",
    "        Q = self.query_layer(embedded)\n",
    "        V = self.value_layer(embedded)\n",
    "        _, T, A = K.shape\n",
    "        scores = Q @ torch.transpose(K, 1, 2) / (A ** 0.5)\n",
    "        mask = torch.tril(torch.ones(T, T))\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        scores = nn.functional.softmax(scores, dim=-1)\n",
    "        return scores @ V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Headed Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedSelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.attention_heads = nn.ModuleList([\n",
    "            SingleHeadAttention(model_dim, model_dim // num_heads) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.compute = nn.Linear(model_dim, model_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "\n",
    "    def forward(self, embedded: TensorType[float]) -> TensorType[float]:\n",
    "        return self.dropout(self.compute(torch.cat([head(embedded) for head in self.attention_heads], dim=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaNeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_dim: int):\n",
    "        super().__init__()\n",
    "        self.first_linear_layer = nn.Linear(model_dim, model_dim * 4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.second_linear_layer = nn.Linear(model_dim * 4, model_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x: TensorType[float]) -> TensorType[float]:\n",
    "        return self.dropout(self.second_linear_layer(self.relu(self.first_linear_layer(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.mhsa = MultiHeadedSelfAttention(model_dim, num_heads)\n",
    "        self.vanilla_nn = VanillaNeuralNetwork(model_dim)\n",
    "        self.layer_norm_one = nn.LayerNorm(model_dim)\n",
    "        self.layer_norm_two = nn.LayerNorm(model_dim)\n",
    "\n",
    "    def forward(self, embedded: TensorType[float]) -> TensorType[float]:\n",
    "        embedded = embedded + self.mhsa(self.layer_norm_one(embedded))  # Pre-norm + residual connection\n",
    "        embedded = embedded + self.vanilla_nn(self.layer_norm_two(embedded))  # Pre-norm + residual connection\n",
    "        return embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int, context_length: int, model_dim: int, num_blocks: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, model_dim)\n",
    "        self.pos_embedding = nn.Embedding(context_length, model_dim)\n",
    "        self.transformer_blocks = nn.Sequential(*(TransformerBlock(model_dim, num_heads) for _ in range(num_blocks)))\n",
    "        self.layer_norm_three = nn.LayerNorm(model_dim)\n",
    "        self.vocab_projection = nn.Linear(model_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context: TensorType[int]) -> TensorType[float]:\n",
    "        _, T = context.shape\n",
    "        # Compute token and positional embeddings\n",
    "        token_embeddings = self.token_embedding(context)\n",
    "        pos_embedding = self.pos_embedding(torch.arange(T))\n",
    "        embedded = token_embeddings + pos_embedding\n",
    "        output = self.transformer_blocks(embedded)  # Pass through transformer blocks\n",
    "        # Layer norm + projection to vocabulary size\n",
    "        return self.vocab_projection(self.layer_norm_three(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, new_chars: int, context: TensorType[int], context_length: int, int_to_char: dict) -> str:\n",
    "    res = []\n",
    "    for _ in range(new_chars):\n",
    "        # Ensure the context length does not exceed the max length\n",
    "        context = context[:, -context_length:]\n",
    "        # Predict the next character probabilities\n",
    "        prediction = model(context)  # B, T, Vocab_Size\n",
    "        last_time_step = prediction[:, -1, :]  # B, Vocab_Size\n",
    "        # Softmax to get the probabilities for the next character\n",
    "        probabilities = nn.functional.softmax(last_time_step, dim=-1)\n",
    "        # Sample the next character based on probabilities\n",
    "        next_char = torch.multinomial(probabilities, 1)  # Sample and get the index\n",
    "        # Update context with the new character\n",
    "        context = torch.cat((context, next_char), dim=-1)\n",
    "        # Append the next character to the result list\n",
    "        res.append(int_to_char[next_char.item()])\n",
    "    return \"\".join(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "vocab_size = 104\n",
    "context_length = 128\n",
    "model_dim = 252\n",
    "num_blocks = 6\n",
    "num_heads = 6\n",
    "\n",
    "int_to_char = {0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '$', 5: '%', 6: '&', 7: \"'\", 8: '(', 9: ')', 10: '*', 11: '+', 12: ',', 13: '-', 14: '.', 15: '/', 16: '0', 17: '1', 18: '2', 19: '3', 20: '4', 21: '5', 22: '6', 23: '7', 24: '8', 25: '9', 26: ':', 27: ';', 28: '?', 29: 'A', 30: 'B', 31: 'C', 32: 'D', 33: 'E', 34: 'F', 35: 'G', 36: 'H', 37: 'I', 38: 'J', 39: 'K', 40: 'L', 41: 'M', 42: 'N', 43: 'O', 44: 'P', 45: 'Q', 46: 'R', 47: 'S', 48: 'T', 49: 'U', 50: 'V', 51: 'W', 52: 'X', 53: 'Y', 54: 'Z', 55: '[', 56: ']', 57: '_', 58: 'a', 59: 'b', 60: 'c', 61: 'd', 62: 'e', 63: 'f', 64: 'g', 65: 'h', 66: 'i', 67: 'j', 68: 'k', 69: 'l', 70: 'm', 71: 'n', 72: 'o', 73: 'p', 74: 'q', 75: 'r', 76: 's', 77: 't', 78: 'u', 79: 'v', 80: 'w', 81: 'x', 82: 'y', 83: 'z', 84: '{', 85: '|', 86: '}', 87: 'à', 88: 'á', 89: 'è', 90: 'é', 91: 'ë', 92: 'ñ', 93: 'ó', 94: 'ú', 95: '\\u2005', 96: '–', 97: '—', 98: '‘', 99: '’', 100: '“', 101: '”', 102: '…', 103: '\\u205f'}\n",
    "char_to_int = {char: idx for idx, char in int_to_char.items()}\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, text):\n",
    "        self.data = [char_to_int[char] for char in text if char in char_to_int]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - context_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx:idx + context_length], dtype=torch.long)\n",
    "        y = torch.tensor(self.data[idx + 1:idx + 1 + context_length], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = GPT(vocab_size, context_length, model_dim, num_blocks, num_heads)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "# Load dataset and create DataLoader\n",
    "text = \"\"\n",
    "with open(\"Donald-Tweets!.csv\", newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    text = \" \".join(row[\"Tweet_Text\"] for row in reader if row[\"Tweet_Text\"])\n",
    "dataset = TweetDataset(text)\n",
    "dataloader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (x, y) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output.view(-1, vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if batch_idx % 10 == 0:  # Print every 10 batches\n",
    "            print(f\"Epoch {epoch + 1} of {num_epochs}, Batch {batch_idx} of {len(dataloader)}, Loss: {loss.item():.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "# Save trained weights\n",
    "torch.save(model.state_dict(), \"trained_weights.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(vocab_size, context_length, model_dim, num_blocks, num_heads)\n",
    "WEIGHT_PATH = 'trained_weights.pt'\n",
    "model.load_state_dict(torch.load(WEIGHT_PATH, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "new_chars = 280  # 5000\n",
    "context = torch.zeros(1, 1, dtype = torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will be talking about jobs like no Washington D.C. http://t.co/jZzfqUZNYh On my way to Cedar Falls, Iowa. Will be great to see you tomorrow - AWESOME made a big person! Get out and VOTE!\n",
      "DrainTheSwamp WE will DrainTheSwamp in Washington D.C. at 10:30 A.M.  MAKE AMERICA GREAT AGAI\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, new_chars,context,\n",
    "               context_length,\n",
    "               int_to_char))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
