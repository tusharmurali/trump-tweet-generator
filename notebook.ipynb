{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SingleHeadAttention(nn.Module):\n",
    "        \n",
    "    def __init__(self, model_dim: int, head_size: int):\n",
    "        super().__init__()\n",
    "        self.key_layer = nn.Linear(model_dim, head_size, bias=False)\n",
    "        self.query_layer = nn.Linear(model_dim, head_size, bias=False)\n",
    "        self.value_layer = nn.Linear(model_dim, head_size, bias=False)\n",
    "    \n",
    "    def forward(self, embedded):\n",
    "        K = self.key_layer(embedded)\n",
    "        Q = self.query_layer(embedded)\n",
    "        V = self.value_layer(embedded)\n",
    "        _, T, A = K.shape\n",
    "        scores = Q @ torch.transpose(K, 1, 2) / (A ** 0.5)\n",
    "        mask = torch.tril(torch.ones(T, T))\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        scores = nn.functional.softmax(scores, dim=-1)\n",
    "        return scores @ V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Headed Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedSelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.attention_heads = nn.ModuleList([\n",
    "            SingleHeadAttention(model_dim, model_dim // num_heads) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.compute = nn.Linear(model_dim, model_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "\n",
    "    def forward(self, embedded):\n",
    "        return self.dropout(self.compute(torch.cat([head(embedded) for head in self.attention_heads], dim=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaNeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_dim: int):\n",
    "        super().__init__()\n",
    "        self.first_linear_layer = nn.Linear(model_dim, model_dim * 4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.second_linear_layer = nn.Linear(model_dim * 4, model_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.second_linear_layer(self.relu(self.first_linear_layer(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.mhsa = MultiHeadedSelfAttention(model_dim, num_heads)\n",
    "        self.vanilla_nn = VanillaNeuralNetwork(model_dim)\n",
    "        self.layer_norm_one = nn.LayerNorm(model_dim)\n",
    "        self.layer_norm_two = nn.LayerNorm(model_dim)\n",
    "\n",
    "    def forward(self, embedded):\n",
    "        embedded = embedded + self.mhsa(self.layer_norm_one(embedded))  # Pre-norm + residual connection\n",
    "        embedded = embedded + self.vanilla_nn(self.layer_norm_two(embedded))  # Pre-norm + residual connection\n",
    "        return embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int, context_length: int, model_dim: int, num_blocks: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, model_dim)\n",
    "        self.pos_embedding = nn.Embedding(context_length, model_dim)\n",
    "        self.transformer_blocks = nn.Sequential(*(TransformerBlock(model_dim, num_heads) for _ in range(num_blocks)))\n",
    "        self.layer_norm_three = nn.LayerNorm(model_dim)\n",
    "        self.vocab_projection = nn.Linear(model_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context):\n",
    "        _, T = context.shape\n",
    "        # Compute token and positional embeddings\n",
    "        token_embeddings = self.token_embedding(context)\n",
    "        pos_embedding = self.pos_embedding(torch.arange(T))\n",
    "        embedded = token_embeddings + pos_embedding\n",
    "        output = self.transformer_blocks(embedded)  # Pass through transformer blocks\n",
    "        # Layer norm + projection to vocabulary size\n",
    "        return self.vocab_projection(self.layer_norm_three(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, new_chars: int, context, context_length: int, int_to_char: dict) -> str:\n",
    "    res = []\n",
    "    for i in range(new_chars):\n",
    "        # Ensure the context length does not exceed the max length\n",
    "        context = context[:, -context_length:]\n",
    "        # Predict the next character probabilities\n",
    "        prediction = model(context)  # B, T, Vocab_Size\n",
    "        last_time_step = prediction[:, -1, :]  # B, Vocab_Size\n",
    "        # Softmax to get the probabilities for the next character\n",
    "        probabilities = nn.functional.softmax(last_time_step, dim=-1)\n",
    "        # Sample the next character based on probabilities\n",
    "        next_char = torch.multinomial(probabilities, 1)  # Sample and get the index\n",
    "        # Update context with the new character\n",
    "        context = torch.cat((context, next_char), dim=-1)\n",
    "        # Append the next character to the result list\n",
    "        res.append(int_to_char[next_char.item()])\n",
    "    return \"\".join(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "vocab_size = 124\n",
    "context_length = 128\n",
    "model_dim = 252\n",
    "num_blocks = 6\n",
    "num_heads = 6\n",
    "\n",
    "int_to_char = {0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '#', 5: '$', 6: '%', 7: '&', 8: \"'\", 9: '(', 10: ')', 11: '*', 12: '+', 13: ',', 14: '-', 15: '.', 16: '/', 17: '0', 18: '1', 19: '2', 20: '3', 21: '4', 22: '5', 23: '6', 24: '7', 25: '8', 26: '9', 27: ':', 28: ';', 29: '=', 30: '?', 31: '@', 32: 'A', 33: 'B', 34: 'C', 35: 'D', 36: 'E', 37: 'F', 38: 'G', 39: 'H', 40: 'I', 41: 'J', 42: 'K', 43: 'L', 44: 'M', 45: 'N', 46: 'O', 47: 'P', 48: 'Q', 49: 'R', 50: 'S', 51: 'T', 52: 'U', 53: 'V', 54: 'W', 55: 'X', 56: 'Y', 57: 'Z', 58: '[', 59: ']', 60: '_', 61: 'a', 62: 'b', 63: 'c', 64: 'd', 65: 'e', 66: 'f', 67: 'g', 68: 'h', 69: 'i', 70: 'j', 71: 'k', 72: 'l', 73: 'm', 74: 'n', 75: 'o', 76: 'p', 77: 'q', 78: 'r', 79: 's', 80: 't', 81: 'u', 82: 'v', 83: 'w', 84: 'x', 85: 'y', 86: 'z', 87: '{', 88: '|', 89: '}', 90: '~', 91: 'à', 92: 'á', 93: 'è', 94: 'é', 95: 'ë', 96: 'ñ', 97: 'ó', 98: 'ú', 99: 'ʉ', 100: '̱', 101: 'ω', 102: 'я', 103: 'ӕ', 104: 'ԍ', 105: 'ԏ', 106: 'Ԡ', 107: 'ե', 108: 'լ', 109: 'ջ', 110: 'ُ', 111: '٪', 112: '\\u06dd', 113: 'ۢ', 114: '۪', 115: '\\u2005', 116: '–', 117: '—', 118: '‘', 119: '’', 120: '“', 121: '”', 122: '…',\n",
    "               123: '\\u205f'}\n",
    "char_to_int = {char: idx for idx, char in int_to_char.items()}\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, text):\n",
    "        self.data = [char_to_int[char] for char in text if char in char_to_int]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - context_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx:idx + context_length], dtype=torch.long)\n",
    "        y = torch.tensor(self.data[idx + 1:idx + 1 + context_length], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = GPT(vocab_size, context_length, model_dim, num_blocks, num_heads)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "# Load dataset and create DataLoader\n",
    "text = \"\"\n",
    "with open(\"Donald-Tweets!.csv\", newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    text = \" \".join(row[\"Tweet_Text\"] for row in reader if row[\"Tweet_Text\"])\n",
    "dataset = TweetDataset(text)\n",
    "dataloader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (x, y) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output.view(-1, vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if batch_idx % 10 == 0:  # Print every 10 batches\n",
    "            print(f\"Epoch {epoch + 1} of {num_epochs}, Batch {batch_idx} of {len(dataloader)}, Loss: {loss.item():.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "# Save trained weights\n",
    "torch.save(model.state_dict(), \"trained_weights.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(vocab_size, context_length, model_dim, num_blocks, num_heads)\n",
    "WEIGHT_PATH = 'trained_weights.pt'\n",
    "model.load_state_dict(torch.load(WEIGHT_PATH, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "new_chars = 280  # 5000\n",
    "context = torch.zeros(1, 1, dtype = torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Crooked Hillary said, she would be going to Rocket Birmston. How can she run? \"@AprilLaJune: OREGON votes today! Go vote for @realDonaldTrump and kick it BIG TIME!  #MAGA #Debates  __\" \"@ihatematt: @realDonaldTrump @megynkelly it makes me not watch the debate by a poll\" \"@apollo\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, new_chars, context,\n",
    "               context_length,\n",
    "               int_to_char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(vocab_size, context_length, model_dim, num_blocks, num_heads)\n",
    "WEIGHT_PATH = 'trained_weights.pt'\n",
    "model.load_state_dict(torch.load(WEIGHT_PATH, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "dummy_input = torch.randint(0, vocab_size, (1, context_length), dtype=torch.int32)\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    dummy_input, \n",
    "    \"model.onnx\", \n",
    "    input_names=[\"input\"], \n",
    "    output_names=[\"output\"], \n",
    "    dynamic_axes={\"input\": {0: \"batch_size\", 1: \"seq_len\"}, \"output\": {0: \"batch_size\", 1: \"seq_len\"}}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
